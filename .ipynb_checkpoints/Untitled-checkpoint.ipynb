{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from  data_folder_split import mkdir ,des_path,load_image\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_dataset(list_path, output_dir):\n",
    "    fd = open(list_path)\n",
    "    lines=[]\n",
    "    for line in fd:\n",
    "        line=line.split(',')\n",
    "        lines.append(line)\n",
    "    fd.close()\n",
    "    output_path = os.path.join(output_dir,\n",
    "        'data_{}.csv'.format('train'))\n",
    "    print(output_path)\n",
    "    #batch_size=len(lines)\n",
    "    batch_size=5000\n",
    "    save_arr = np.empty((batch_size, 128*128+1), dtype=np.str)\n",
    "    #save_arr = pd.DataFrame(save_arr)\n",
    "    for i in range(batch_size):\n",
    "        image_data =open(lines[i][1], 'rb')\n",
    "        image = load_image(image_data)\n",
    "        image=np.append(image,int(lines[i][2].replace('\\n','')))\n",
    "        save_arr=np.row_stack((save_arr,image))\n",
    "        image_data.close()\n",
    "    save_arr = pd.DataFrame(save_arr) \n",
    "    save_arr.to_csv(output_path, decimal=',', encoding='utf-8', index=False, index_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/TMD/npz_train/float_train/\n",
      "folder is exist\n",
      "../data/TMD/npz_train/float_eval/\n",
      "folder is exist\n"
     ]
    }
   ],
   "source": [
    "TF_train_file_dir=des_path+'float_train/'\n",
    "TF_eval_file_dir=des_path+'float_eval/'\n",
    "mkdir(TF_train_file_dir)\n",
    "mkdir(TF_eval_file_dir)\n",
    "list_train_file_name=des_path+'list_train.txt'\n",
    "list_eval_file_name=des_path+'list_val.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/TMD/npz_train/float_train/data_train.csv\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fda2b63eefad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconvert_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_train_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTF_train_file_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconvert_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_eval_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTF_eval_file_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-df029a1f7d11>\u001b[0m in \u001b[0;36mconvert_dataset\u001b[1;34m(list_path, output_dir)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msave_arr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_arr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mimage_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0msave_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \"\"\"\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "convert_dataset(list_train_file_name, TF_train_file_dir)\n",
    "convert_dataset(list_eval_file_name, TF_eval_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from data_folder_split import load_image \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "FLAGS = None\n",
    "\n",
    "data_file = '../data/TMD/npz_train/float_train/data_train.csv'\n",
    "\n",
    "train_data=pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16375</th>\n",
       "      <th>16376</th>\n",
       "      <th>16377</th>\n",
       "      <th>16378</th>\n",
       "      <th>16379</th>\n",
       "      <th>16380</th>\n",
       "      <th>16381</th>\n",
       "      <th>16382</th>\n",
       "      <th>16383</th>\n",
       "      <th>16384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9  ...    16375  16376  16377  16378  \\\n",
       "0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  ...      NaN    NaN    NaN    NaN   \n",
       "1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  ...      NaN    NaN    NaN    NaN   \n",
       "2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  ...      NaN    NaN    NaN    NaN   \n",
       "3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  ...      NaN    NaN    NaN    NaN   \n",
       "4 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  ...      NaN    NaN    NaN    NaN   \n",
       "\n",
       "   16379  16380  16381  16382  16383  16384  \n",
       "0    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "1    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "2    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "3    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "4    NaN    NaN    NaN    NaN    NaN    NaN  \n",
       "\n",
       "[5 rows x 16385 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/TMD/npz_train/train/data_00000-of-00005.tfrecord'\n",
    "record_iterator=tf.python_io.tf_record_iterator(path=data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def records_to(file, num_threads=2, num_epochs=2, batch_size=30, min_after_dequeue=1000):\n",
    "    file_queue = tf.train.string_input_producer([file],num_epochs=num_epochs,)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, example = reader.read(file_queue)\n",
    "    features_dict = tf.parse_single_example(example, \n",
    "        features={\n",
    "            'image/class/label': tf.FixedLenFeature([], tf.int64),\n",
    "            'image/encoded': tf.FixedLenFeature([], tf.string),\n",
    "            'image/format': tf.FixedLenFeature([], tf.string),\n",
    "            'image/height': tf.FixedLenFeature([], tf.string),\n",
    "            'image/width': tf.FixedLenFeature([], tf.string)\n",
    "        })\n",
    "    # n: Tensor(\"ParseSingleExample/Squeeze_name:0\", shape=(), dtype=string)\n",
    "    label =  tf.cast(features_dict['image/class/label'],tf.int32)\n",
    "    image = tf.decode_raw(features_dict['image/encoded'], tf.uint8)\n",
    "    image = tf.cast(image, tf.float32) - 0.5\n",
    "    image_format =  tf.cast(features_dict['image/format'],tf.string)\n",
    "    height =  tf.cast(features_dict['image/height'],tf.int32)\n",
    "    width =  tf.cast(features_dict['image/width'],tf.int32)\n",
    "    print (label)\n",
    "    print (image)\n",
    "    print (image_format)\n",
    "    print (height)\n",
    "    print (width)\n",
    "    label_batch, image_batch, image_format_batch,height_batch,width_batch = tf.train.shuffle_batch(\n",
    "        [label, image, image_format,height,width],\n",
    "        batch_size=30,\n",
    "        num_threads=2,\n",
    "        capacity =2000,\n",
    "        min_after_dequeue = 1000)\n",
    "    # 数据格式为Tensor\n",
    "    return label_batch, image_batch, image_format_batch,height_batch,width_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    label, image, image_format,height,width = records_to(data_file)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.group(tf.global_variables_initializer(),\n",
    "            tf.local_variables_initializer()).run()\n",
    "        tf.train.start_queue_runners(sess=sess)\n",
    "        label_val, image_val, image_format_val , height_val, width_val \\\n",
    "        = sess.run([label, image, image_format,height,width])\n",
    "        print(label_val, image_val, image_format_val , height_val, width_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast_27:0\", shape=(), dtype=int32) Tensor(\"sub_11:0\", shape=(?,), dtype=float32) Tensor(\"ParseSingleExample_12/Squeeze_image/format:0\", shape=(), dtype=string) Tensor(\"Cast_30:0\", shape=(), dtype=int32) Tensor(\"Cast_31:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All shapes must be fully defined: [TensorShape([]), TensorShape([Dimension(None)]), TensorShape([]), TensorShape([]), TensorShape([])]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-fe3bad8f98a9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecords_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         tf.group(tf.global_variables_initializer(),\n",
      "\u001b[1;32m<ipython-input-69-06a9c23ec876>\u001b[0m in \u001b[0;36mrecords_to\u001b[1;34m(file, num_threads, num_epochs, batch_size, min_after_dequeue)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mcapacity\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         min_after_dequeue = 1000)\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m# 数据格式为Tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_format_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheight_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\u001b[0m in \u001b[0;36mshuffle_batch\u001b[1;34m(tensors, batch_size, capacity, min_after_dequeue, num_threads, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)\u001b[0m\n\u001b[0;32m   1285\u001b[0m       \u001b[0mallow_smaller_final_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_smaller_final_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m       \u001b[0mshared_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1287\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m   1288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py\u001b[0m in \u001b[0;36m_shuffle_batch\u001b[1;34m(tensors, batch_size, capacity, min_after_dequeue, keep_input, num_threads, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)\u001b[0m\n\u001b[0;32m    821\u001b[0m     queue = data_flow_ops.RandomShuffleQueue(\n\u001b[0;32m    822\u001b[0m         \u001b[0mcapacity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_after_dequeue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_after_dequeue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m         dtypes=types, shapes=shapes, shared_name=shared_name)\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[0m_enqueue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menqueue_many\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m     full = (math_ops.to_float(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, capacity, min_after_dequeue, dtypes, shapes, names, seed, shared_name, name)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \"\"\"\n\u001b[0;32m    656\u001b[0m     \u001b[0mdtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_type_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_shape_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m     \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_name_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[0mseed1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\u001b[0m in \u001b[0;36m_as_shape_list\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     76\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0munknown_dim_allowed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All shapes must be fully defined: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0munknown_rank_allowed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All shapes must be fully defined: [TensorShape([]), TensorShape([Dimension(None)]), TensorShape([]), TensorShape([]), TensorShape([])]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
